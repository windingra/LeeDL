{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34954,"databundleVersionId":3300998,"sourceType":"competition"}],"dockerImageVersionId":30171,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you want to access the version you have already modified, click \"Edit\"\n# If you want to access the original sample code, click \"...\", then click \"Copy & Edit Notebook\"","metadata":{}},{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":19.351342,"end_time":"2022-02-23T10:03:06.247288","exception":false,"start_time":"2022-02-23T10:02:46.895946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:16:13.602358Z","iopub.execute_input":"2025-11-19T05:16:13.602765Z","iopub.status.idle":"2025-11-19T05:16:42.316883Z","shell.execute_reply.started":"2025-11-19T05:16:13.602641Z","shell.execute_reply":"2025-11-19T05:16:42.315776Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"papermill":{"duration":0.0189,"end_time":"2022-02-23T10:03:06.279758","exception":false,"start_time":"2022-02-23T10:03:06.260858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T03:28:50.148445Z","iopub.execute_input":"2025-11-19T03:28:50.148802Z","iopub.status.idle":"2025-11-19T03:28:50.155133Z","shell.execute_reply.started":"2025-11-19T03:28:50.148753Z","shell.execute_reply":"2025-11-19T03:28:50.153955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":1.654263,"end_time":"2022-02-23T10:03:07.947242","exception":false,"start_time":"2022-02-23T10:03:06.292979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:16:58.621605Z","iopub.execute_input":"2025-11-19T05:16:58.621936Z","iopub.status.idle":"2025-11-19T05:17:00.463863Z","shell.execute_reply.started":"2025-11-19T05:16:58.621904Z","shell.execute_reply":"2025-11-19T05:17:00.462722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.078771,"end_time":"2022-02-23T10:03:08.039428","exception":false,"start_time":"2022-02-23T10:03:07.960657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T03:28:56.035617Z","iopub.execute_input":"2025-11-19T03:28:56.036011Z","iopub.status.idle":"2025-11-19T03:28:56.047265Z","shell.execute_reply.started":"2025-11-19T03:28:56.035977Z","shell.execute_reply":"2025-11-19T03:28:56.045949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Transforms**\nTorchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n\nPlease refer to PyTorch official website for details about different transforms.","metadata":{"papermill":{"duration":0.01289,"end_time":"2022-02-23T10:03:08.065357","exception":false,"start_time":"2022-02-23T10:03:08.052467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    # You may add some transforms here.\n    \n    # 2. 几何变换 (增加数据多样性)\n    transforms.RandomHorizontalFlip(p=0.5),        # 水平翻转\n    transforms.RandomRotation(15),                 # 随机旋转 +/- 15度\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # 随机平移\n    \n    # 3. 像素变换 (防止过拟合颜色)\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    \n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])\n","metadata":{"papermill":{"duration":0.021406,"end_time":"2022-02-23T10:03:08.099437","exception":false,"start_time":"2022-02-23T10:03:08.078031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:17:20.816599Z","iopub.execute_input":"2025-11-19T05:17:20.816901Z","iopub.status.idle":"2025-11-19T05:17:20.825459Z","shell.execute_reply.started":"2025-11-19T05:17:20.816872Z","shell.execute_reply":"2025-11-19T05:17:20.824361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Datasets**\nThe data is labelled by the name, so we load images and label while calling '__getitem__'","metadata":{"papermill":{"duration":0.012739,"end_time":"2022-02-23T10:03:08.125181","exception":false,"start_time":"2022-02-23T10:03:08.112442","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files is not None: #为k折提供的接口\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n\n","metadata":{"papermill":{"duration":0.023022,"end_time":"2022-02-23T10:03:08.160912","exception":false,"start_time":"2022-02-23T10:03:08.13789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:17:22.977290Z","iopub.execute_input":"2025-11-19T05:17:22.978433Z","iopub.status.idle":"2025-11-19T05:17:22.989050Z","shell.execute_reply.started":"2025-11-19T05:17:22.978324Z","shell.execute_reply":"2025-11-19T05:17:22.987887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n\n            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n\n            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n\n            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n            \n            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)\n        out = out.view(out.size()[0], -1)\n        return self.fc(out)\n\n# [Step 2] 使用 ResNet18 模型\nimport torchvision.models as models\n\ndef get_model():\n    # 加载 ResNet18 结构，不使用预训练权重\n    model = models.resnet18(pretrained=False)\n    \n    # 修改全连接层 (fc)，因为 ResNet18 默认输出 1000 类，我们需要 11 类\n    # model.fc.in_features 是 ResNet18 最后一层的输入维度 (通常是 512)\n    model.fc = nn.Linear(model.fc.in_features, 11) \n    return model\n\n# 验证一下模型是否能跑\n# print(get_model())","metadata":{"papermill":{"duration":0.0258,"end_time":"2022-02-23T10:03:08.199437","exception":false,"start_time":"2022-02-23T10:03:08.173637","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:17:28.474931Z","iopub.execute_input":"2025-11-19T05:17:28.475323Z","iopub.status.idle":"2025-11-19T05:17:28.488071Z","shell.execute_reply.started":"2025-11-19T05:17:28.475269Z","shell.execute_reply":"2025-11-19T05:17:28.487114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_dataset_dir = \"../input/ml2022spring-hw3b/food11\"","metadata":{"papermill":{"duration":0.054295,"end_time":"2022-02-23T10:03:08.266338","exception":false,"start_time":"2022-02-23T10:03:08.212043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:17:32.321143Z","iopub.execute_input":"2025-11-19T05:17:32.321461Z","iopub.status.idle":"2025-11-19T05:17:32.326959Z","shell.execute_reply.started":"2025-11-19T05:17:32.321428Z","shell.execute_reply":"2025-11-19T05:17:32.325854Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# [Step 3] 准备 K-Fold 交叉验证\nfrom sklearn.model_selection import KFold\nimport glob\n\n# 1. 收集所有数据路径 (合并 Training 和 Validation)\n# 注意：请确保 _dataset_dir 变量已定义，例如 \"../input/ml2022spring-hw3b/food11\"\ntrain_dir = os.path.join(_dataset_dir, \"training\")\nval_dir = os.path.join(_dataset_dir, \"validation\")\n\n# 拿出所有 jpg 文件路径\nall_train_files = sorted([os.path.join(train_dir, x) for x in os.listdir(train_dir) if x.endswith(\".jpg\")])\nall_val_files = sorted([os.path.join(val_dir, x) for x in os.listdir(val_dir) if x.endswith(\".jpg\")])\nall_files = np.array(all_train_files + all_val_files) # 合并成一个大池子\n\n# 2. 设置参数\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nn_epochs = 80       # [Train Longer] 建议设为 80-200，取决于你的时间。Simple Baseline只有4。\npatience = 15       # Early Stopping 容忍度\nbatch_size = 64\nn_folds = 5         # 5折交叉验证\nseed = 6666\n\n# 3. 开始 K-Fold 循环\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\n# 为了节省时间，演示代码里你可以只跑一个 Fold，或者把 range(n_folds) 改成 range(1)\n# 如果要跑满 Strong Baseline，需要跑完所有 Fold\nfor fold_idx, (train_idx, val_idx) in enumerate(kf.split(all_files)):\n    \n    print(f\"\\n----- Starting Fold {fold_idx + 1} / {n_folds} -----\")\n    \n    # 划分当前 Fold 的文件\n    fold_train_files = all_files[train_idx]\n    fold_val_files = all_files[val_idx]\n    \n    # 构建 Dataset 和 DataLoader\n    # 注意：我们复用 FoodDataset，利用 files 参数传入文件列表\n    train_set = FoodDataset(path=train_dir, tfm=train_tfm, files=fold_train_files)\n    valid_set = FoodDataset(path=val_dir, tfm=test_tfm, files=fold_val_files) # 验证集不用增强\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n    \n    # 初始化模型 (每个 Fold 都要全新的模型)\n    model = get_model().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n    \n    best_acc = 0\n    stale = 0\n    \n    # 训练循环\n    for epoch in range(n_epochs):\n        # --- Training ---\n        model.train()\n        train_loss = []\n        train_accs = []\n        for batch in tqdm(train_loader, desc=f\"Fold {fold_idx+1} Epoch {epoch+1}\"):\n            imgs, labels = batch\n            logits = model(imgs.to(device))\n            loss = criterion(logits, labels.to(device))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n            train_loss.append(loss.item())\n            train_accs.append(acc)\n            \n        avg_train_loss = sum(train_loss) / len(train_loss)\n        avg_train_acc = sum(train_accs) / len(train_accs)\n        \n        # --- Validation ---\n        model.eval()\n        valid_loss = []\n        valid_accs = []\n        with torch.no_grad():\n            for batch in valid_loader:\n                imgs, labels = batch\n                logits = model(imgs.to(device))\n                loss = criterion(logits, labels.to(device))\n                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n                valid_loss.append(loss.item())\n                valid_accs.append(acc)\n                \n        avg_valid_loss = sum(valid_loss) / len(valid_loss)\n        avg_valid_acc = sum(valid_accs) / len(valid_accs)\n        \n        print(f\"[Fold {fold_idx+1} | {epoch+1}/{n_epochs}] Train Acc: {avg_train_acc:.4f}, Valid Acc: {avg_valid_acc:.4f}\")\n        \n        # 保存最佳模型 (每个 Fold 单独保存)\n        if avg_valid_acc > best_acc:\n            print(f\"New Best Fold {fold_idx+1} Model! Acc: {avg_valid_acc:.4f}\")\n            torch.save(model.state_dict(), f\"{_exp_name}_fold{fold_idx+1}_best.ckpt\")\n            best_acc = avg_valid_acc\n            stale = 0\n        else:\n            stale += 1\n            if stale > patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break","metadata":{"papermill":{"duration":32830.720158,"end_time":"2022-02-23T19:10:19.001001","exception":false,"start_time":"2022-02-23T10:03:08.280843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-04T07:49:49.005042Z","iopub.execute_input":"2022-03-04T07:49:49.005532Z","iopub.status.idle":"2022-03-04T07:57:48.780573Z","shell.execute_reply.started":"2022-03-04T07:49:49.005491Z","shell.execute_reply":"2022-03-04T07:57:48.779782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing and generate prediction CSV","metadata":{"papermill":{"duration":0.498773,"end_time":"2022-02-23T19:10:20.961802","exception":false,"start_time":"2022-02-23T19:10:20.463029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# [Step 4] 极简版：集成预测 (Ensemble)\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\n# 准备测试集\ntest_set = FoodDataset(os.path.join(_dataset_dir, \"test\"), tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n\n# 初始化全零数组 [样本数, 11个类别]\nfinal_logits = np.zeros((len(test_set), 11))\n\n# 遍历每一个 Fold 的模型\nfor fold_idx in range(n_folds):\n    model_path = f\"{_exp_name}_fold{fold_idx+1}_best.ckpt\"\n    \n    if not os.path.exists(model_path):\n        print(f\"Model {model_path} not found, skipping.\")\n        continue\n        \n    print(f\"Inference with {model_path}...\")\n    \n    # 加载模型\n    model = get_model().to(device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    \n    fold_preds = []\n    with torch.no_grad():\n        for data, _ in tqdm(test_loader):\n            logits = model(data.to(device))\n            fold_preds.append(logits.cpu().numpy())\n    \n    # 直接累加 Logits\n    final_logits += np.concatenate(fold_preds)\n\n# 直接对“和”取最大值索引 (不需要除以模型数量)\nprediction = np.argmax(final_logits, axis=1)\n","metadata":{"papermill":{"duration":49.157727,"end_time":"2022-02-23T19:11:10.61523","exception":false,"start_time":"2022-02-23T19:10:21.457503","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-11-19T05:17:36.905350Z","iopub.execute_input":"2025-11-19T05:17:36.905660Z","iopub.status.idle":"2025-11-19T05:17:36.943021Z","shell.execute_reply.started":"2025-11-19T05:17:36.905626Z","shell.execute_reply":"2025-11-19T05:17:36.941742Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)\nprint(\"Ensemble submission saved!\")","metadata":{"papermill":{"duration":0.554276,"end_time":"2022-02-23T19:11:11.870035","exception":false,"start_time":"2022-02-23T19:11:11.315759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-04T07:58:26.537894Z","iopub.execute_input":"2022-03-04T07:58:26.53815Z","iopub.status.idle":"2022-03-04T07:58:26.56908Z","shell.execute_reply.started":"2022-03-04T07:58:26.538114Z","shell.execute_reply":"2022-03-04T07:58:26.56829Z"},"trusted":true},"outputs":[],"execution_count":null}]}